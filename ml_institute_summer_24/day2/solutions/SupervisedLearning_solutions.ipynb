{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning \n",
    "\n",
    "# Part 1\n",
    "\n",
    "Part 1 will cover the material cover the ML algorithms from morning lecture:\n",
    "- knn\n",
    "- linear regression \n",
    "- regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import neighbors\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few functions for plotting (feel free to ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_regions_for_classifier(clf, X, y, X_test=None, y_test=None, title=None, target_names = None, plot_decision_regions = True):\n",
    "\n",
    "    numClasses = np.amax(y) + 1\n",
    "    color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n",
    "    color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n",
    "    cmap_light = ListedColormap(color_list_light[0:numClasses])\n",
    "    cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n",
    "\n",
    "    h = 0.03\n",
    "    k = 0.5\n",
    "    x_plot_adjust = 0.1\n",
    "    y_plot_adjust = 0.1\n",
    "    plot_symbol_size = 50\n",
    "\n",
    "    x_min = X[:, 0].min()\n",
    "    x_max = X[:, 0].max()\n",
    "    y_min = X[:, 1].min()\n",
    "    y_max = X[:, 1].max()\n",
    "    x2, y2 = np.meshgrid(np.arange(x_min-k, x_max+k, h), np.arange(y_min-k, y_max+k, h))\n",
    "\n",
    "    P = clf.predict(np.c_[x2.ravel(), y2.ravel()])\n",
    "    P = P.reshape(x2.shape)\n",
    "    plt.figure()\n",
    "    if plot_decision_regions:\n",
    "        plt.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n",
    "    plt.xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n",
    "    plt.ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n",
    "\n",
    "    if (X_test is not None):\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n",
    "        train_score = clf.score(X, y)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    if (target_names is not None):\n",
    "        legend_handles = []\n",
    "        for i in range(0, len(target_names)):\n",
    "            patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n",
    "            legend_handles.append(patch)\n",
    "        plt.legend(loc=0, handles=legend_handles)\n",
    "\n",
    "    if (title is not None):\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "def plot_two_class_knn(X, y, n_neighbors, weights, X_test, y_test):\n",
    "    X_mat = X\n",
    "    y_mat = y\n",
    "\n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['#FFFFAA', '#AAFFAA', '#AAAAFF','#EFEFEF'])\n",
    "    cmap_bold  = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X_mat, y_mat)\n",
    "\n",
    "    # Plot the decision boundary by assigning a color in the color map\n",
    "    # to each mesh point.\n",
    "    \n",
    "    mesh_step_size = .01  # step size in the mesh\n",
    "    plot_symbol_size = 50\n",
    "    \n",
    "    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1\n",
    "    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size),\n",
    "                         np.arange(y_min, y_max, mesh_step_size))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light,shading='auto')\n",
    "\n",
    "    # Plot training points\n",
    "    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    title = \"Neighbors = {}\".format(n_neighbors)\n",
    "    if (X_test is not None):\n",
    "        train_score = clf.score(X_mat, y_mat)\n",
    "        test_score  = clf.score(X_test, y_test)\n",
    "        title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n",
    "\n",
    "    patch0 = mpatches.Patch(color='#FFFF00', label='class 0')\n",
    "    patch1 = mpatches.Patch(color='#000000', label='class 1')\n",
    "    plt.legend(handles=[patch0, patch1])\n",
    "\n",
    "    plt.xlabel('Feature 0')\n",
    "    plt.ylabel('Feature 1')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighobur classification \n",
    "\n",
    "For this example, we use the iris data set. https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\n",
    "\n",
    "The data set consists of 150 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and print description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and print information about it \n",
    "\n",
    "iris = datasets.load_iris() # get data\n",
    "\n",
    "print('The iris.data has {} samples each of which has {} features. e.g. first two data: {}'\n",
    "      .format(iris.data.shape[0], iris.data.shape[1], iris.data[:2]))\n",
    "print('The iris.target has lables (one of {}) for {} samples. e.g. first two labels {}'\n",
    "      .format(iris.target_names, iris.target.shape[0], iris.target[:2]))\n",
    "\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get selected features and targets from iris data set for training\n",
    "\n",
    "X represents the features and y represents the targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels={0:'setosa',1:'versicolor',2:'virginica'}\n",
    "fig,ax=plt.subplots()\n",
    "for i in range(3):\n",
    "    ax.scatter(X[y==i, 0], X[y==i, 1],label=labels[i])\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_title('Iris Dataset')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a KNN Classification Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # import model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is sklearn's classification report.  This includes alternative classification metrics to accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report  # includes various metrics for evaluating classification models\n",
    "y_pred=knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # computes and visually displays the confusion matrix \n",
    "cm=confusion_matrix(y_pred, y_test) #,labels=['setosa','versicolor','virginica'])\n",
    "cm_display = ConfusionMatrixDisplay(cm,display_labels=['setosa','versicolor','virginica']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with Model \n",
    "\n",
    "Finally, we may want to use our model to make predictions on unseen data points.  Below, we demo how this can be done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [[5.9,  3.], [4.4, 1.9]]\n",
    "predictions= knn.predict(examples)\n",
    "\n",
    "print('Predicted iris type for ', examples, ' is ', \n",
    "          [iris.target_names[x] for x in predictions ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL EXERCISE\n",
    "\n",
    "In the demo above we trained and evaluated a KNN model with k=10.  Using the Iris dataset as defined in variables X and Y above, train a KNN model with k = 1 and k = 100 and compute the accuracy of each model with testing data.  What is the best value of k: 1,10 or 50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models with 1 nearest neighbor \n",
    "knn1 = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn1.fit(X_train, y_train)\n",
    "\n",
    "# compute accuracy\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn1.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn1.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models with 1 nearest neighbor \n",
    "knn50 = KNeighborsClassifier(n_neighbors = 50)\n",
    "knn50.fit(X_train, y_train)\n",
    "\n",
    "# compute accuracy\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn50.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn50.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Decision Boundaries \n",
    "\n",
    "### Decision boundaries with synthetic data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset for classification (binary) \n",
    "plt.figure()\n",
    "plt.title('Sample binary classification problem with two features')\n",
    "\n",
    "# create synthetic dataset \n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "\n",
    "# plot synthetic dataset\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000']) \n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n",
    "           marker= 'o', s=50, cmap=cmap_bold)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state=0)\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 5, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 10, 'uniform', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL EXERCISE\n",
    "\n",
    "By visually inspecting the data above, what is the best value of k-- 1, 5 or 10? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "The model where k=5 is the best model.  The model where k=1 appears to overfit the data, as islands appear around rara data points.  The model where k=10 seems to underfit the data, particularly on the right side of the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression with synthetic binary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make and plot synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data \n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "\n",
    "# plot data \n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test split and model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8, label='training data')\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-', label='model')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression with the diabetes housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression with Boston Housing dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "print('The diabetes.data has {} samples each of which has {} features. e.g. first two data: {}'\n",
    "      .format(diabetes.data.shape[0], diabetes.data.shape[1], diabetes.data[:2]))\n",
    "print('The diabetes.target has values for {} samples. e.g. first two labels {}'\n",
    "      .format(diabetes.target.shape[0], diabetes.target[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring our data \n",
    "\n",
    "In linear regression, it is good to explore the relationships between your features and targets to see if linear relationships exist or can be engineered. Below we explore this via a few data visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(diabetes.target, bins=25)\n",
    "plt.xlabel(\"quantitative measure of disease progression one year after baseline\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(diabetes.data[:, 0:1] , diabetes.target, marker='o')\n",
    "plt.title(\"Variation in House prices\")\n",
    "plt.xlabel(\"standardized age\")\n",
    "plt.ylabel(\"quantitative measure of disease progression one year after baseline\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(diabetes.data[:, 2:3] , diabetes.target, marker='o')\n",
    "plt.title(\"Variation in House prices\")\n",
    "plt.xlabel(\"standardized bmi\")\n",
    "plt.ylabel(\"quantitative measure of disease progression one year after baseline\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train and evaluate model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2,\n",
    "                                                   random_state = 33)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Boston House dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL EXERCISE\n",
    "\n",
    "How does simple linear regression compare to multiple linear regression?  In the model above we used all features available.  However, we see in our EDA that there wasn't necessarily a strong linear relationship between all of our features and our target.  \n",
    "\n",
    "Try building a simple linear regression model where our features is body mass index.  Compute the R^2 coefficient for this model. How do these results compare to the multiple linear regression model we built in the previous cell?\n",
    "\n",
    "\n",
    "To help get you started, the test/train split is already set up for below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data[:, 2:3], diabetes.target, test_size=0.2,\n",
    "                                                   random_state = 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Boston House dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Regression\n",
    "\n",
    "Below we use Ridge Regression to train a model with the Boston Housing dataset.  It is important to standardize your data when using regularization to ensure that the beta coefficients you find are all of similar magnitude.  \n",
    "Below we choose to standardize or data with the MinMaxScalar.  Please note that there are two functions used, fit_transform and transform.  By performing the fit, we are computing the min and max values of the data provided and storing these values so that the can used to transform data.  Never fit the MinMaxScaler with testing data -- only use training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2,\n",
    "                                                   random_state = 33)\n",
    "\n",
    "# Note we typically would need to standardize our data.  however, this data comes pre-standardized\n",
    "# code below is one way we could standardize our data with the min max scaler described above\n",
    "\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "'''\n",
    "\n",
    "linreg = Ridge(alpha=1.0).fit(X_train, y_train)\n",
    "\n",
    "print('Boston House dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is the best value of alpha to use? \n",
    "\n",
    "An important hyperparameter when using Ridge regression is alpha.  Below we consider several values of alpha so that we can find optimal performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the best value of alpha to use? \n",
    "r2s = []\n",
    "alphas=[0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "for alpha in alphas:\n",
    "    linreg = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    r2s.append( linreg.score(X_test, y_test) )\n",
    "    print('Alpha: {} R-squared score (test): {:.3f}'.format(alpha,r2s[-1]))\n",
    "    \n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(alphas,r2s)\n",
    "ax.scatter(alphas,r2s)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('R^2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Excercise \n",
    "\n",
    "Copy and modify the code above to see how the performance changes when we use Lasso Regression instead of Ridge for various values of alpha.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# what is the best value of alpha to use? \n",
    "r2s = []\n",
    "alphas=[0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "for alpha in alphas:\n",
    "    linreg = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "    r2s.append( linreg.score(X_test, y_test) )\n",
    "    print('Alpha: {} R-squared score (test): {:.3f}'.format(alpha,r2s[-1]))\n",
    "    \n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(alphas,r2s)\n",
    "ax.scatter(alphas,r2s)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('R^2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 \n",
    "\n",
    "The remainder of the machine learning methods will be covered in the afternoon session:\n",
    "\n",
    "- logistic regression \n",
    "- support vector machines\n",
    "- naive bayes\n",
    "- decision tree based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression with breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "print('The cancer.data has {} samples each of which has {} features. e.g. first two data: {}'\n",
    "      .format(cancer.data.shape[0], cancer.data.shape[1], cancer.data[:2]))\n",
    "print('The cancer.target has lables (one of {}) for {} samples. e.g. first two labels {}'\n",
    "      .format(cancer.target_names, cancer.target.shape[0], cancer.target[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate logistic regression model\n",
    "\n",
    "Note the parameter C when training the Logistic Regression is similar to alpha used in ridge regression.  However, C and alpha have an inverse relation ship.  The smaller the value of C, the stronger the regularization is.  The regularization term is why standardizing the data is needed when training this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "# Logistic regression for binary classification )\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer[:, 20:30], y_cancer,\n",
    "                                                   random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine\n",
    "# Linear Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = LinearSVC().fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of Linear SVC classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Linear SVC classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you likely received a ConvergenceWarning from the cell above.  We included this cell to highlight the fact that this is another example of where standardizing data can help.  In general, standardized data makes the training process easier, specifically when your training process involves using a numerical optimizer to optimize a loss function.  Below we train the model again, but with standardized data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "clf = LinearSVC().fit(X_train_scaled, y_train)\n",
    "print('Breast cancer dataset (normalized with MinMax scaling)')\n",
    "print('Polynomial-kernel (degree=3) SVC (with MinMax scaling) training set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('Polynomial-kernel (degree=3) SVC (with MinMax scaling) test set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = ConfusionMatrixDisplay.from_estimator(clf, X_test_scaled, y_test,\n",
    "                                                   display_labels=cancer.target_names,\n",
    "                                                   cmap=plt.cm.Blues,normalize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification with linear models and SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification with linear models\n",
    "# LinearSVC with M classes generates M one vs rest classifiers.\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LinearSVC(C=5, random_state = 67).fit(X_train_scaled, y_train)\n",
    "print('Coefficients:\\n', clf.coef_)\n",
    "print('Intercepts:\\n', clf.intercept_)\n",
    "print('Accuracy of Linear SVC classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of Linear SVC classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_display = ConfusionMatrixDisplay.from_estimator(clf, X_test_scaled, y_test,\n",
    "                                                   display_labels=iris.target_names,\n",
    "                                                   cmap=plt.cm.Blues,normalize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs and the kernel trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBF Kernel normlized \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = SVC(kernel='rbf',C=10).fit(X_train_scaled, y_train)\n",
    "print('Breast cancer dataset (normalized with MinMax scaling)')\n",
    "print('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL EXERCISE\n",
    "\n",
    "Try building another SVC model except instead of using the rbf kernel, use the polynomial kernel of degree 5:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "How do the results compare? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly',degree=5,C=10).fit(X_train_scaled, y_train)\n",
    "print('Breast cancer dataset (normalized with MinMax scaling)')\n",
    "print('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2,\n",
    "                       centers = 8, cluster_std = 1.3,\n",
    "                       random_state = 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test,\n",
    "                                'Gaussian Naive Bayes classifier: Dataset 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer[:, 0:10] , y_cancer, random_state = 0)\n",
    "nbclf = GaussianNB().fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of GaussianNB classifier on training set: {:.2f}'\n",
    "     .format(nbclf.score(X_train, y_train)))\n",
    "print('Accuracy of GaussianNB classifier on test set: {:.2f}'\n",
    "     .format(nbclf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "\n",
    "### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 3)\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting maximum depth\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn import tree\n",
    "import importlib\n",
    "import matplotlib as mpl\n",
    "import pprint\n",
    "importlib.reload(mpl); importlib.reload(plt); importlib.reload(sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']\n",
    "cn=['setosa', 'versicolor', 'virginica']\n",
    "fig,ax=plt.subplots(dpi=300)\n",
    "output=tree.export_text(clf, feature_names = fn ) #, class_names=cn,filled = True)\n",
    "\n",
    "print(output)\n",
    "\n",
    "print('Note labels:',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(clf, \n",
    "                   feature_names=iris.feature_names,  \n",
    "                   class_names=iris.target_names,\n",
    "                   filled=True)\n",
    "plt.savefig('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "\n",
    "### Random blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2,\n",
    "                       centers = 8, cluster_std = 1.3,\n",
    "                       random_state = 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "print('Accuracy of Random Forest classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Random Forest classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "print('Accuracy of RandomForest classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of RandomForest classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note :  If you want to visualize your trees in the random forest you can as demoed below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer= load_breast_cancer()\n",
    "_ = tree.plot_tree(clf.estimators_[1],\n",
    "                   filled=True)\n",
    "plt.savefig('rf_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
